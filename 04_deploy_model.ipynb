{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48c365a-0e35-4ff9-9164-fc69e9f9f608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/personalized_image_gen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d95e6c7-4cab-4310-a2bd-0e5407fe1860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Create a model serving endpoint with Python\n",
    "Now we have a fine-tuned model registered in Unity Catalog, our final step is to deploy this model behind a Model Serving endpoint. This notebook covers wrapping the REST API queries for model serving endpoint creation, updating endpoint configuration based on model version, and endpoint deletion with Python for your Python model serving workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d15b8ab-9058-4c03-8e40-26b116122f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the registry URI to Databricks Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22201418-76d0-448a-99e9-768db11f8899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Specify some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214ea04b-870e-4dde-a4b2-6368791fbd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "theme = \"chair\"\n",
    "catalog = \"sdxl_image_gen\"\n",
    "log_schema = \"log\" # A schema within the catalog where the inferece log is going to be stored \n",
    "model_name = f\"{catalog}.model.sdxl-fine-tuned-{theme}\"  # An existing model in model registry, may have multiple versions\n",
    "model_serving_endpoint_name = f\"sdxl-fine-tuned-{theme}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6397e0-c004-4e31-8279-b5752c6ab09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up configurations\n",
    "Depending on the latency and throughput requirements of your use case, you want to choose the right `workload_type` and `workload_size`. **Note that if you're using Azure Databricks, use `GPU_LARGE` for `workload_type`**. The `auto_capture_config` block specifies where to write the inference logs: i.e. requests and responses from the endpoint with a timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b6817d-a7eb-4b7d-918f-1ab9b14a6003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the champion model version\n",
    "champion_version = client.get_model_version_by_alias(model_name, \"champion\")\n",
    "model_version = champion_version.version\n",
    "\n",
    "my_json = {\n",
    "    \"name\": model_serving_endpoint_name,\n",
    "    \"config\": {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_version\": model_version,\n",
    "                \"workload_type\": \"GPU_MEDIUM\",\n",
    "                \"workload_size\": \"Small\",\n",
    "                \"scale_to_zero_enabled\": \"false\",\n",
    "            }\n",
    "        ],\n",
    "        \"auto_capture_config\": {\n",
    "            \"catalog_name\": catalog,\n",
    "            \"schema_name\": log_schema,\n",
    "            \"table_name_prefix\": model_serving_endpoint_name,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Make sure to the schema for the inference table exists\n",
    "_ = spark.sql(\n",
    "    f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{log_schema}\"\n",
    ")\n",
    "\n",
    "# Make sure to drop the inference table of it exists\n",
    "_ = spark.sql(\n",
    "    f\"DROP TABLE IF EXISTS {catalog}.{log_schema}.`{model_serving_endpoint_name}_payload`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c55fd0-b4d1-434c-bd76-e159674064c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following defines Python functions that:\n",
    "- create a model serving endpoint\n",
    "- update a model serving endpoint configuration with the latest model version\n",
    "- delete a model serving endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0b3dac-a664-4413-b13b-811ed7795a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "\n",
    "def func_create_endpoint(json):\n",
    "    client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    try:\n",
    "        # Check if the endpoint already exists\n",
    "        client.get_deployment(json[\"name\"])\n",
    "        # Update the existing endpoint with the new model version\n",
    "        client.update_deployment(\n",
    "            name=json[\"name\"], \n",
    "            config=json[\"config\"]\n",
    "        )\n",
    "    except:\n",
    "        # Create a new endpoint if it doesn't exist\n",
    "        client.create_endpoint(\n",
    "            name = model_serving_endpoint_name,\n",
    "            config = json[\"config\"],\n",
    "        )\n",
    "\n",
    "def func_delete_model_serving_endpoint(json):\n",
    "    client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    # Delete the specified endpoint\n",
    "    client.delete_endpoint(json[\"name\"])\n",
    "    print(json[\"name\"], \"endpoint is deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa50373c-111d-4711-8aee-2224b375cd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "func_create_endpoint(my_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f88b544-0123-4307-9ac8-85660849e4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wait for the endpoint to be ready\n",
    "\n",
    "The `wait_for_endpoint()` function defined in the following command gets and returns the serving endpoint status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41364e1c-45d5-4968-bed3-b65c68e95bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def wait_for_endpoint(endpoint_name):\n",
    "    '''Wait for a model serving endpoint to be ready'''\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "    import time\n",
    "\n",
    "    # Initialize WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    state = \"\"\n",
    "    for i in range(200):\n",
    "        state = w.serving_endpoints.get(endpoint_name).state\n",
    "        if state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "            if i % 40 == 0:\n",
    "                print(f\"Waiting for endpoint to deploy {endpoint_name}. Current state: {state}\")\n",
    "            time.sleep(10)\n",
    "        elif state.ready == EndpointStateReady.READY:\n",
    "            print('endpoint ready.')\n",
    "            return\n",
    "        else:\n",
    "            break\n",
    "    raise Exception(f\"Couldn't start the endpoint, timeout, please check your endpoint for more details: {state}\")\n",
    "\n",
    "wait_for_endpoint(my_json[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4496aef4-f2fc-4175-83b7-df9eccbaae89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Score the model\n",
    "The following command defines the `score_model()` function  and an example scoring request under the `payload_json` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b82c10-a1ff-4097-9dad-a82508a8b1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "def generate_image(endpoint, dataset):\n",
    "    # Initialize the MLflow deployment client for Databricks\n",
    "    client = get_deploy_client(\"databricks\")\n",
    "    \n",
    "    # Convert the dataset to a dictionary in 'split' orientation\n",
    "    ds_dict = {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n",
    "    \n",
    "    # Make a prediction request to the specified endpoint with the dataset\n",
    "    response = client.predict(endpoint=endpoint, inputs=ds_dict)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c812c9f0-e1e4-4c46-8b57-b5a0a391884d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame with the prompt and number of inference steps\n",
    "prompt = pd.DataFrame(\n",
    "    {\"prompt\": [\"A photo of an orange bcnchr chair\"], \"num_inference_steps\": 25}\n",
    ")\n",
    "\n",
    "# Generate image using the specified endpoint and prompt\n",
    "t = generate_image(my_json[\"name\"], prompt)\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(t[\"predictions\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310f5d1e-826e-4bbf-a1e7-e088a7fceb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61803ee-4fe7-483f-a533-c9688ec2b2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "func_delete_model_serving_endpoint(my_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f48306-87b7-4e61-969f-1ed0bc78763a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| bitsandbytes | Accessible large language models via k-bit quantization for PyTorch. | MIT | https://pypi.org/project/bitsandbytes/\n",
    "| diffusers | A library for pretrained diffusion models for generating images, audio, etc. | Apache 2.0 | https://pypi.org/project/diffusers/\n",
    "| stable-diffusion-xl-base-1.0 | A model that can be used to generate and modify images based on text prompts. | CreativeML Open RAIL++-M License | https://github.com/Stability-AI/generative-models"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_deploy_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
